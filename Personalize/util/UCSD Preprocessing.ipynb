{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-(before-the-workshop)-processing of UCSD Data\n",
    "\n",
    "This solution and workshop use the [Amazon Review Data (2018)](https://nijianmo.github.io/amazon/index.html) dataset published by UCSD, as used in the paper:\n",
    "\n",
    "**Justifying recommendations using distantly-labeled reviews and fined-grained aspects**<br/>\n",
    "Jianmo Ni, Jiacheng Li, Julian McAuley<br/>\n",
    "_Empirical Methods in Natural Language Processing (EMNLP), 2019 [(PDF)](http://cseweb.ucsd.edu/~jmcauley/pdfs/emnlp19a.pdf)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "The base dataset is:\n",
    "\n",
    "1. **Large** - We're only trying to create a representative demo here, not needlessly consume lots of resources\n",
    "2. **Sparse** - Since views > purchases > reviews, only a fraction of products (and users) have multiple reviews\n",
    "\n",
    "UCSD's published data already helps with this, by offering **5-core reviews** subsets pre-filtered to include only the reviews:\n",
    "\n",
    "* by users who've written 5 or more reviews\n",
    "* for products that received 5 or more reviews\n",
    "\n",
    "If we focus on one particular product category (say, _\"Sports and Outdoors\"_ or _\"Grocery and Gourmet Food\"_) then this gets us to a nice manageable volume of reviews for training demo recommender engines - great!\n",
    "\n",
    "...The only remaining problem is that there **aren't 5-core filtered versions of the product metadata files**... So our demo's start-up time (populating the products database) would still be unacceptable! ðŸ˜­\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "\n",
    "Here we simply **filter out (most of) the un-reviewed products from a UCSD category product metadata file**: Creating a slimmed-down product metadata file which is what will be used by the demo solution.\n",
    "\n",
    "* Assuming the parsed 5-core reviews dataset fits in to memory (so lookups are performant), but the full category product list doesn't (so streaming is necessary)\n",
    "* Preserving the data format (so users can relate it to the source dataset, and the workshop can still do some of the more \"real\" data pre-processing)\n",
    "\n",
    "**Why \"most of\", not all?** We'd still like *some* unreviewed products, to demonstrate cold-start functionality... We just don't want our website swamped with 'em!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Imports, AWS connection, and configuration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import io\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "\n",
    "# Local Dependencies:\n",
    "from preproc import remove_unused_items\n",
    "from dataformat import json_gz_reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "s3 = session.resource(\"s3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we only mirror a subset of the files available through\n",
    "# https://nijianmo.github.io/amazon/index.html\n",
    "category = \"Grocery_and_Gourmet_Food\"\n",
    "max_cold_start = 0.01\n",
    "\n",
    "reviews_uri = f\"s3://public-personalize-demo-assets-{region}/data/{category}_5.json.gz\"\n",
    "products_raw_uri = f\"s3://public-personalize-demo-assets-{region}/data/meta_{category}.json.gz\"\n",
    "products_out_uri = \\\n",
    "    f\"s3://public-personalize-demo-assets-{region}/data/meta_{category}_{5 + max_cold_start}.json.gz\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Reviewed Product IDs\n",
    "\n",
    "Loading the interactions file from S3 directly into RAM might take a while depending on the size of the category.\n",
    "\n",
    "We only need the set of product IDs mentioned in any review, so that's all we store.\n",
    "\n",
    "*Grocery_and_Gourmet_Food_5.json.gz* (1.1M reviews) took ~14s on our t3 instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reviewed_item_ids = set()\n",
    "n_reviews = 0\n",
    "\n",
    "reviews_bucket, _, reviews_key = reviews_uri[len(\"s3://\"):].partition(\"/\")\n",
    "\n",
    "for review in json_gz_reader(s3.Object(reviews_bucket, reviews_key).get()[\"Body\"]):\n",
    "    n_reviews += 1\n",
    "    reviewed_item_ids.add(review[\"asin\"])\n",
    "\n",
    "print(f\"{len(reviewed_item_ids)} products reviewed over {n_reviews} reviews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the Product Metadata\n",
    "\n",
    "Now just need to filter out the majority of the un-reviewed products. You should see from the console output below just how extreme the data reduction is!\n",
    "\n",
    "We buffer the filtered binary data into memory (why create files everywhere?) and then directly upload it to S3.\n",
    "\n",
    "(On our t3 instance with *meta_Grocery_and_Gourmet_Food.json.gz*, this took around 50s end-to-end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "products_raw_bucket, _, products_raw_key = products_raw_uri[len(\"s3://\"):].partition(\"/\")\n",
    "products_out_bucket, _, products_out_key = products_out_uri[len(\"s3://\"):].partition(\"/\")\n",
    "\n",
    "# See the local file this function was imported from above for implementation details.\n",
    "# The output file should fit in memory, so let's not pollute the filesystem.\n",
    "with io.BytesIO() as fout:\n",
    "    print(f\"Filtering data from {products_raw_uri}...\")\n",
    "    remove_unused_items(\n",
    "        s3.Object(products_raw_bucket, products_raw_key).get()[\"Body\"],\n",
    "        fout,\n",
    "        reviewed_item_ids,\n",
    "        max_cold_start=max_cold_start,\n",
    "    )\n",
    "\n",
    "    fout.seek(0)\n",
    "    print(f\"\\nUploading to {products_out_uri}...\")\n",
    "    s3.Object(products_out_bucket, products_out_key).put(Body=fout)\n",
    "    print(\"Uploaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all! It would be possible to validate this work by e.g:\n",
    "\n",
    "- Re-running `remove_unused_items()` on the filtered product dataset to check the result is the same\n",
    "- Iterating through the filtered product dataset to check the item format is the same\n",
    "\n",
    "...We could even `pandas.read_json(..., lines=True)` on the output S3 URI, but for most product categories we'd still need more RAM than a `ml.t*.medium` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
