{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Recommendation Model Types\n",
    "\n",
    "So far we've used Amazon Personalize's `HRNN` model to deploy a `user->items` recommendation model - probably the first use case that comes to mind when thinking about recommendation engines.\n",
    "\n",
    "...But there are two other use cases Personalize can help with on our website:\n",
    "\n",
    "1. Recommending items **in the context of an item**: i.e. \"Similar items\", or more accurately \"Users also bought/viewed/etc.\"\n",
    "2. **Re-ranking a list of items** for relevance to a particular user: i.e. prioritizing impressions we want to make from a base set.\n",
    "\n",
    "In this notebook, we'll use our previously imported data to train and deploy both types - powering further personalization on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "\n",
    "# Local Dependencies:\n",
    "import util\n",
    "\n",
    "%store -r role_arn\n",
    "%store -r dataset_group_arn\n",
    "%store -r hrnn_campaign_arn\n",
    "\n",
    "personalize = boto3.client(\"personalize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Similar\" Items\n",
    "\n",
    "Given a particular item in context (e.g. currently viewing the detail page for a product, or just added a product to the basket), we'd like to recommend related items that might drive additional transactions.\n",
    "\n",
    "Of course our standard HRNN based user->item model is stateful and so will adapt to the user's session to an extent, but here we're aiming for a more explicit relationship from a certain item.\n",
    "\n",
    "The model we train here will be deployed in the **\"Similar Items\"** section of the product detail page on our website - which also includes a **\"Recommended For You\"** section based on the user->item model, so we'll be able to see how the two compare in action.\n",
    "\n",
    "### Creating the Solution Version\n",
    "\n",
    "Our dataset(s) have already been loaded into Amazon Personalize for the previous model, so (as we did for HRNN) the first step is to create a Solution based on the `SIMS` recipe and train a Solution Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_arn = \"arn:aws:personalize:::recipe/aws-sims\"\n",
    "\n",
    "create_solution_response = personalize.create_solution(\n",
    "    name=f\"{os.environ['CF_STACK_NAME']}-soln-sims\",\n",
    "    datasetGroupArn=dataset_group_arn,\n",
    "    recipeArn=sims_arn\n",
    ")\n",
    "\n",
    "sims_solution_arn = create_solution_response[\"solutionArn\"]\n",
    "print(json.dumps(create_solution_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_solution_version_response = personalize.create_solution_version(\n",
    "    solutionArn=sims_solution_arn\n",
    ")\n",
    "\n",
    "sims_solution_version_arn = create_solution_version_response[\"solutionVersionArn\"]\n",
    "print(json.dumps(create_solution_version_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if you'd like to resume with an existing solution version ARN taken from the console:\n",
    "#sims_solution_version_arn = \"arn:aws:personalize:us-east-1:387269085412:solution/thewsfooda-soln-sims/68311135\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_version_is_ready(description):\n",
    "    status = description[\"solutionVersion\"][\"status\"]\n",
    "    if status == \"ACTIVE\":\n",
    "        return True\n",
    "    elif \"FAILED\" in status:\n",
    "        raise ValueError(\n",
    "            f\"Wait ended with unexpected status '{status}':\\n{description}\"\n",
    "        )\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(f\"Waiting for solution version {sims_solution_version_arn}...\")\n",
    "util.progress.polling_spinner(\n",
    "    lambda: personalize.describe_solution_version(solutionVersionArn=sims_solution_version_arn),\n",
    "    solution_version_is_ready,\n",
    "    fn_stringify_result = lambda desc: desc[\"solutionVersion\"][\"status\"],\n",
    "    timeout_secs=3*60*60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution metrics\n",
    "\n",
    "As with HRNN, we can view metrics generated by Personalize to get an idea of the model's performance.\n",
    "\n",
    "As with HRNN, these metrics are only indicative and the best way to understand business impact will be a live A/B test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn=sims_solution_version_arn\n",
    ")\n",
    "\n",
    "print(json.dumps(solution_metrics_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the solution\n",
    "\n",
    "Once the solution is trained, we'll deploy it to a **Campaign** and then configure our website by setting a **Lambda Function environment variable**.\n",
    "\n",
    "This time we're configuring the `LAMBDA_GETITEMRECS_ARN` function, as opposed to the `LAMBDA_GETRECS_ARN` function which powers user->item recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_campaign_response = personalize.create_campaign(\n",
    "    name=f\"{os.environ['CF_STACK_NAME']}-camp-sims\",\n",
    "    solutionVersionArn=sims_solution_version_arn,\n",
    "    minProvisionedTPS=1,\n",
    ")\n",
    "\n",
    "sims_campaign_arn = create_campaign_response[\"campaignArn\"]\n",
    "%store sims_campaign_arn\n",
    "print(json.dumps(create_campaign_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if you'd like to resume with an existing campaign ARN taken from the console:\n",
    "# sims_campaign_arn = \"arn:aws:personalize:us-east-1:387269085412:campaign/thewsfooda-camp-sims\"\n",
    "# %store sims_campaign_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "\n",
    "print(f\"Waiting for campaign {sims_campaign_arn}...\")\n",
    "details = {}\n",
    "status = \"UNKNOWN\"\n",
    "# TODO: Stricter status check\n",
    "while status not in (\"ACTIVE\", \"CREATE FAILED\"):\n",
    "    if status != \"UNKNOWN\":\n",
    "        time.sleep(60)\n",
    "\n",
    "    details = personalize.describe_campaign(campaignArn=sims_campaign_arn)\n",
    "    status = details[\"campaign\"][\"status\"]\n",
    "\n",
    "if status != \"ACTIVE\":\n",
    "    raise ValueError(f\"Wait ended with unexpected status '{status}':\\n{details}\")\n",
    "else:\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.lambdafn.update_lambda_envvar(os.environ[\"LAMBDA_GETITEMRECS_ARN\"], \"CAMPAIGN_ARN\", sims_campaign_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see recommendations in the \"Similar Items\" section of product detail pages on your website: How do they look? How do they differ from the standard HRNN-driven recommendations in the \"Recommended for You\" section of the same pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-ranking items\n",
    "\n",
    "For our final use case, we'd like to take a proposed list of items and **re-prioritize** them depending on what's expected to be most relevant to our user.\n",
    "\n",
    "For an interactive demo on the website, we'll use this to promote certain items to the top of *search results*.\n",
    "\n",
    "In practice search is a non-trivial use case for these models, because we need to strike a compromise between attending to the user's requests (as interpreted by the search engine in raw item relevance scores) and the personalization aspects (driven by the **history of interactions** rather than the current request). For demo purposes we take a simplified approach to this: promoting up to N results to the top of the list, and passing the remaining results through as ordered by the search engine.\n",
    "\n",
    "**Other use cases** for this type of model could include prioritizing a shortlist of campaigns generated by rule-based or manual methods: e.g. selecting which campaigns/messages/items to show to each user from a list curated by marketing and segmented by demographics or other customer data.\n",
    "\n",
    "### Creating the Solution Version\n",
    "\n",
    "Again we'll create a solution and train a solution version, this time referencing the \"Personalized Ranking\" recipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_arn = \"arn:aws:personalize:::recipe/aws-personalized-ranking\"\n",
    "\n",
    "create_solution_response = personalize.create_solution(\n",
    "    name=f\"{os.environ['CF_STACK_NAME']}-soln-rerank\",\n",
    "    datasetGroupArn=dataset_group_arn,\n",
    "    recipeArn=rerank_arn\n",
    ")\n",
    "\n",
    "rerank_solution_arn = create_solution_response[\"solutionArn\"]\n",
    "print(json.dumps(create_solution_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_solution_version_response = personalize.create_solution_version(\n",
    "    solutionArn=rerank_solution_arn\n",
    ")\n",
    "\n",
    "rerank_solution_version_arn = create_solution_version_response[\"solutionVersionArn\"]\n",
    "print(json.dumps(create_solution_version_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if you'd like to resume with an existing solution version ARN taken from the console:\n",
    "#rerank_solution_version_arn = \"arn:aws:personalize:us-east-1:387269085412:solution/thewsfooda-soln-rerank/68311135\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Waiting for solution version {rerank_solution_version_arn}...\")\n",
    "util.progress.polling_spinner(\n",
    "    lambda: personalize.describe_solution_version(solutionVersionArn=rerank_solution_version_arn),\n",
    "    solution_version_is_ready,\n",
    "    fn_stringify_result = lambda desc: desc[\"solutionVersion\"][\"status\"],\n",
    "    timeout_secs=3*60*60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution metrics\n",
    "\n",
    "...You know the drill by now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn=rerank_solution_version_arn\n",
    ")\n",
    "\n",
    "print(json.dumps(solution_metrics_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the solution\n",
    "\n",
    "Just like the other two models, but this time we're configuring the `LAMBDA_RERANK_ARN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_campaign_response = personalize.create_campaign(\n",
    "    name=f\"{os.environ['CF_STACK_NAME']}-camp-rerank\",\n",
    "    solutionVersionArn=rerank_solution_version_arn,\n",
    "    minProvisionedTPS=1,\n",
    ")\n",
    "\n",
    "rerank_campaign_arn = create_campaign_response[\"campaignArn\"]\n",
    "%store rerank_campaign_arn\n",
    "print(json.dumps(create_campaign_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if you'd like to resume with an existing campaign ARN taken from the console:\n",
    "# rerank_campaign_arn = \"arn:aws:personalize:us-east-1:387269085412:campaign/thewsfooda-camp-rerank\"\n",
    "# %store rerank_campaign_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "\n",
    "print(f\"Waiting for campaign {rerank_campaign_arn}...\")\n",
    "details = {}\n",
    "status = \"UNKNOWN\"\n",
    "# TODO: Stricter status check\n",
    "while status not in (\"ACTIVE\", \"CREATE FAILED\"):\n",
    "    if status != \"UNKNOWN\":\n",
    "        time.sleep(60)\n",
    "\n",
    "    details = personalize.describe_campaign(campaignArn=rerank_campaign_arn)\n",
    "    status = details[\"campaign\"][\"status\"]\n",
    "\n",
    "if status != \"ACTIVE\":\n",
    "    raise ValueError(f\"Wait ended with unexpected status '{status}':\\n{details}\")\n",
    "else:\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.lambdafn.update_lambda_envvar(os.environ[\"LAMBDA_RERANK_ARN\"], \"CAMPAIGN_ARN\", rerank_campaign_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Search results* on your website should now be ranked differently, depending which user is \"signed in\" in the dropdown menu. How do the new result rankings compare to raw search results? How could you **balance** promoting items a user's history suggests they'd be interested in, with those most relevant to their current search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And that's it!\n",
    "\n",
    "Using the same core dataset and with just slightly tweaked API calls, we've deployed two more kinds of recommendations on our website to drive engagement and revenue.\n",
    "\n",
    "...But there's one big question we've not really covered so far: improving model quality. In the next notebook, we'll show how additional metadata and hyperparameter tuning can help."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
