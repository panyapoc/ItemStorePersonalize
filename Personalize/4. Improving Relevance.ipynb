{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Relevance of Recommendations\n",
    "\n",
    "Our focus so far has been on functionality: We've deployed 3 types of recommendation model to our website (and enabled real-time event feedback via the `personalize-events` SDK) using the same original interactions dataset of just user ID, item ID, and timestamp.\n",
    "\n",
    "So how can we improve the **accuracy/relevance** of our deployed models to boost performance?\n",
    "\n",
    "This notebook demonstrates two tools available:\n",
    "\n",
    "1. Adding **metadata** to help the model generalize and even tackle \"cold start\" problem of recommending new items with no data.\n",
    "2. Performing **hyperparameter optimization (HPO)** to tune the model's performance as best we can on given datasets.\n",
    "\n",
    "The relative gains available are strongly dataset-dependent. In this notebook we'll start by focussing on our source data, and then bring in HPO second.\n",
    "\n",
    "First, let's get our library imports and initial setup out of the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We'll use this library for progress bars later, and it's not installed by default:\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import csv\n",
    "from datetime import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore import exceptions as botoexceptions\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local Dependencies:\n",
    "import util\n",
    "\n",
    "%store -r role_arn\n",
    "%store -r dataset_group_arn\n",
    "%store -r hrnn_solution_arn\n",
    "%store -r hrnn_campaign_arn\n",
    "\n",
    "personalize = boto3.client(\"personalize\")\n",
    "rekognition = boto3.client(\"rekognition\")  # For optional item data enrichment from image\n",
    "s3 = boto3.resource(\"s3\")  # Cloud object storage (for our data!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding metadata\n",
    "\n",
    "[Amazon Personalize](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html) supports 3 historical and one live-updating type of dataset in each **dataset group**:\n",
    "\n",
    "* **Interactions (Required)**: Observed `user-item` interactions are at the core of Personalize's modelling philosophy, which focusses on directly analysing patterns to predict future interactions, rather than proxying via assumed labels like user demographics or item categories. In this respect, HRNN-based models could be considered as an extension over the traditional field of [Collaborative Filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)\n",
    "* **Items (Optional)**: Item metadata which, *secondarily to the core interaction data*, could help the model understand relationships between different item IDs.\n",
    "* **Users (Optional)**: User metadata which, *secondarily to the core interaction data*, coudl help the model understand relationships between different user IDs.\n",
    "* ***Events (Optional)***: *Live additions* to the historical interactions dataset, as collected through the Personalize Events API.\n",
    "\n",
    "Since ([as detailed in the developer guide](https://docs.aws.amazon.com/personalize/latest/dg/recording-events.html)) **Events** should map back to the **Interactions** schema, there are only 3 independent schemas for us to design.\n",
    "\n",
    "All of these schemas support certain [required and reserved fields](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html) (like `ITEM_ID`) - but also **custom** fields.\n",
    "\n",
    "As usual in machine learning we have the *freedom* to define extra fields to help the model (e.g. generalize observed patterns to other users or items with less data) - but the *responsibility* to add only **informative** data. Adding attributes which don't bring **value** to the modelling just introduces noise and makes it *harder* for the model to perform: The [feature selection](https://en.wikipedia.org/wiki/Feature_selection) problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User data\n",
    "\n",
    "As discussed earlier, the [UCSD Amazon Reviews datasets](https://nijianmo.github.io/amazon/index.html) our example is based on don't have any user metadata beyond the simple user ID - so we **won't create a \"Users\" dataset** in Amazon Personalize as we don't have any data.\n",
    "\n",
    "To explore this option with custom datasets, you can use our **Item** metadata steps as a guide since the process is very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction metadata\n",
    "\n",
    "Interaction metadata is a little special because there are two ways it can be used:\n",
    "\n",
    "* To help the model **better understand patterns** in the historic data, and/or;\n",
    "* To provide **context** already known at the point we generate recommendations for users (see the [\"Getting Real-Time Recommendations\"](https://docs.aws.amazon.com/personalize/latest/dg/getting-real-time-recommendations.html) doc)\n",
    "\n",
    "Contextual information such as device type or location might help us adapt recommendations in real-time, whereas retrospective information like whether the customer left a review for a purchase would only be useful for understanding historic patterns.\n",
    "\n",
    "Let's take another look at the raw interaction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_interactions = 0\n",
    "sample_interaction = None\n",
    "\n",
    "for interaction in util.dataformat.data_folder_reader(\"data/raw/interactions\"):\n",
    "    n_interactions += 1\n",
    "\n",
    "    # Reservoir sampling R-algorithm (simple, non-optimal) with k=1:\n",
    "    if random.randint(1, n_interactions) <= 1:\n",
    "        sample_interaction = interaction\n",
    "\n",
    "print(f\"\\nGot {n_interactions} interactions total\")\n",
    "print(\"Sample interaction:\")\n",
    "print(sample_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So is there anything here that might be helpful?\n",
    "\n",
    "User or item attributes like `reviewerName` sometimes feature in interaction data to make analysis easier or because they might change over time - but for our modelling purposes are probably best left for user and item metadata sets\n",
    "\n",
    "Since the UCSD dataset is **reviews** (rather than views or purchases), we have a whole host of extra information like:\n",
    "\n",
    "* `overall`: a \"stars\" style score from 1.0 to 5.0\n",
    "    * Useful for quantifying the customer's experience of the item\n",
    "    * Likely predictive of repeat purchases *for categories where repeat purchase is likely* (e.g. consumables, short-lifespan)\n",
    "* `vote`: review usefulness vote total (number represented as string - not always present)\n",
    "    * Is the perceived insightfulness of the review relevant for us?\n",
    "    * Potentially could be a marker for inauthentic/fraudulent behaviour\n",
    "* `verified`: whether the reviewer had a verified purchase of the item (boolean)\n",
    "    * Potentially could be a marker for inauthentic/fraudulent behaviour e.g. reviewing competitor or own products\n",
    "* `summary` and `reviewText`: the actual text of the review\n",
    "    * Useful for understanding detailed item feedback\n",
    "    * Length (perhaps relative to users' average review length) could provide further insight into how strong the opinion is\n",
    "\n",
    "At the time of writing Amazon Personalize reserves `EVENT_TYPE` (e.g. review vs purchase vs view) and `EVENT_VALUE` (e.g. purchase price or review rating) fields in the **Interactions** schema: But the standard recipes do not automatically account for events of different types and values to train one composite model.\n",
    "\n",
    "For our example website we'd like to avoid filtering the data (e.g. on only positive reviews) because this would exacerbate the sparsity of the data: We already know users only review a fraction of the products they purchase, and even e.g. leaving a bad review for a bag of ground coffee may indicate the customer's interest in other ground coffee products.\n",
    "\n",
    "...So **we'll add the overall \"stars\" score** as our `EVENT_VALUE` field, recognising that it matches the intended use of the reserved field even though current models may not be affected by it.\n",
    "\n",
    "TODO: EVENT_TYPE too?\n",
    "\n",
    "It could be argued that tagging reviews with some sort of authenticity score (e.g. `verified`) might be relevant for judging historic patterns, but probably it's more important to track down and remove fraudulent behaviour (e.g. using [Amazon Fraud Detector](https://aws.amazon.com/fraud-detector/)) than to improve the product recommendations the fraudsters see. If their interaction patterns are markedly different from typical users, this should already coach the algorithm to treat them differently.\n",
    "\n",
    "There aren't really any fields that could give us useful contextual cues at runtime (e.g. device, location, etc.) - and even if there were, this is a reviews dataset so the review context might be quite different from the purchase context.\n",
    "\n",
    "...Therefore for our example's interactions we'll add the `EVENT_VALUE` field and leave it at that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        { \"name\": \"USER_ID\", \"type\": \"string\" },\n",
    "        { \"name\": \"ITEM_ID\", \"type\": \"string\" },\n",
    "        { \"name\": \"TIMESTAMP\", \"type\": \"long\" },\n",
    "        { \"name\": \"EVENT_TYPE\", \"type\": \"string\" },\n",
    "        { \"name\": \"EVENT_VALUE\", \"type\": \"float\" },\n",
    "    ],\n",
    "    \"version\": \"1.0\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name=f\"{os.environ['CF_STACK_NAME']}-schema-interactions-extended\",\n",
    "        schema=json.dumps(interactions_schema)\n",
    "    )\n",
    "\n",
    "    interactions_schema_arn = create_schema_response[\"schemaArn\"]\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "\n",
    "except botoexceptions.ClientError as err:\n",
    "    # If the schema already exists, scrape the ARN from the message and check it:\n",
    "    if err.response[\"Error\"][\"Code\"] == \"ResourceAlreadyExistsException\":\n",
    "        warnings.warn(\n",
    "            \"Schema already exists.\\nScraping ARN from error message.\\n\"\n",
    "            \"To change the existing schema, delete it and re-create it.\"\n",
    "        )\n",
    "        msg = err.response[\"Error\"][\"Message\"]\n",
    "        interactions_schema_arn = msg[msg.index(\"arn:aws:personalize\"):].partition(\" \")[0]\n",
    "        description = personalize.describe_schema(schemaArn=interactions_schema_arn)\n",
    "        print(description)\n",
    "    else:\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_filename = \"data/interactions-extended.csv\"\n",
    "with open(interactions_filename, \"w\") as f:\n",
    "    writer = csv.DictWriter(\n",
    "        f,\n",
    "        dialect=\"unix\",\n",
    "        fieldnames=[\"USER_ID\", \"ITEM_ID\", \"TIMESTAMP\", \"EVENT_TYPE\", \"EVENT_VALUE\"]\n",
    "    )\n",
    "    writer.writeheader()\n",
    "    print(\"Writing interactions...\")\n",
    "    for event in util.dataformat.data_folder_reader(\"data/raw/interactions\"):\n",
    "        writer.writerow({\n",
    "            \"USER_ID\": util.dataformat.get_interaction_user_id(event),\n",
    "            \"ITEM_ID\": util.dataformat.get_interaction_item_id(event),\n",
    "            \"TIMESTAMP\": util.dataformat.get_interaction_timestamp(event),\n",
    "            \"EVENT_TYPE\": \"review\",\n",
    "            \"EVENT_VALUE\": util.dataformat.get_interaction_value(event),\n",
    "        })\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3:\n",
    "s3.Object(os.environ[\"STAGING_BUCKET\"], interactions_filename).upload_file(interactions_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Typically this will fail because the old dataset already exists:\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name=f\"{os.environ['CF_STACK_NAME']}-interactions\",\n",
    "        datasetType=\"INTERACTIONS\",\n",
    "        datasetGroupArn=dataset_group_arn,\n",
    "        schemaArn=interactions_schema_arn,\n",
    "    )\n",
    "\n",
    "    interactions_dataset_arn = create_dataset_response[\"datasetArn\"]\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "except botoexceptions.ClientError as err:\n",
    "    if err.response[\"Error\"][\"Code\"] == \"ResourceAlreadyExistsException\":\n",
    "        warnings.warn(\"Dataset already exists. Deleting old dataset to update schema...\")\n",
    "        old_dataset_arn = (\n",
    "            dataset_group_arn.replace(\":dataset-group/\", \":dataset/\")\n",
    "            + \"/INTERACTIONS\"\n",
    "        )\n",
    "\n",
    "        # Delete the existing dataset and wait for the deletion to complete:\n",
    "        personalize.delete_dataset(datasetArn=old_dataset_arn)\n",
    "        status=\"DELETE PENDING\"\n",
    "        max_time = time.time() + 20*60 # 20 mins\n",
    "        while status != \"DELETED\":\n",
    "            assert time.time() < max_time, \"Timed out waiting for old dataset to delete\"\n",
    "            try:\n",
    "                description = personalize.describe_dataset(datasetArn=old_dataset_arn)\n",
    "            except botoexceptions.ClientError as err:\n",
    "                if err.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\n",
    "                    print(\"Existing dataset deleted\")\n",
    "                    status = \"DELETED\"\n",
    "                else:\n",
    "                    raise err\n",
    "            time.sleep(15)  # Should only take a short while, so polling can be fast\n",
    "\n",
    "        # Re-create the dataset with the new schema:\n",
    "        create_dataset_response = personalize.create_dataset(\n",
    "            name=f\"{os.environ['CF_STACK_NAME']}-interactions\",\n",
    "            datasetType=\"INTERACTIONS\",\n",
    "            datasetGroupArn=dataset_group_arn,\n",
    "            schemaArn=interactions_schema_arn,\n",
    "        )\n",
    "        print(json.dumps(create_dataset_response, indent=2))\n",
    "    else:  # Some other problem\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    # (strftime %f is microseconds, so we trim 3 from the end for milliseconds)\n",
    "    jobName=f\"{os.environ['CF_STACK_NAME']}-interact-{dt.now().strftime('%Y-%m-%d-%H-%M-%S-%f')[:-3]}\",\n",
    "    datasetArn=interactions_dataset_arn,\n",
    "    dataSource={\n",
    "        \"dataLocation\": f\"s3://{os.environ['STAGING_BUCKET']}/{interactions_filename}\"\n",
    "    },\n",
    "    roleArn=role_arn,  # Remember the IAM role we created earlier?\n",
    ")\n",
    "\n",
    "interactions_dataset_import_job_arn = create_dataset_import_job_response[\"datasetImportJobArn\"]\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That import job will take some time to run, but rather than waiting on it now we'll start preparing and importing our Item data in parallel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item data\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_items = 0\n",
    "sample_item = None\n",
    "\n",
    "for item in util.dataformat.data_folder_reader(\"data/raw/items\"):\n",
    "    n_items += 1\n",
    "\n",
    "    # Reservoir sampling R-algorithm (simple, non-optimal) with k=1:\n",
    "    if random.randint(1, n_items) <= 1:\n",
    "        sample_item = item\n",
    "\n",
    "print(f\"\\nGot {n_items} items total\")\n",
    "print(\"Sample item:\")\n",
    "print(sample_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case there's a lot of potentially interesting data associated to the items - although we need to be careful of some inconsistent/missing fields and some occasional crawling errors in the raw data. (e.g. where website HTML and JavaScript are given in description fields, instead of just the description text.\n",
    "\n",
    "For our purposes, we'll try to create a simple `CATEGORY` field describing each item to improve the model's ability to generalize.\n",
    "\n",
    "There are a couple of ways we might do this:\n",
    "\n",
    "1. If (as for the UCSD data) category information is already provided in the dataset, we could just pass it through to an items CSV file.\n",
    "2. Alternatively, other AI/ML models could be used to derive product features... Such as using [Amazon Rekognition](https://aws.amazon.com/rekognition/) to create category tags based on [DetectLabels](https://docs.aws.amazon.com/rekognition/latest/dg/API_DetectLabels.html) results\n",
    "\n",
    "Here we'll show how to build up an items dataset via both methods **in the notebook**. In real-world architectures this feature engineering would typically be integrated into an **automated pipeline** on new product registration: Just like our CloudFormation stack automatically indexes products in ElasticSearch when they're added to the website's DynamoDB table.\n",
    "\n",
    "In both cases (since multiple category labels might apply to a product) we'll use the format for multi-value categorical fields as detailed in the [Amazon Personalize docs](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html): bar-separated values along the lines of `category1|category2|category3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Using existing categories\n",
    "\n",
    "This method is simple enough, as we're simply constructing the CSV from the features already provided in the raw UCSD data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_cats_per_item = 3\n",
    "\n",
    "# We set this differently between the two methods, so you won't overwrite by default:\n",
    "items_filename = \"data/items-basic.csv\"\n",
    "\n",
    "with open(items_filename, \"w\") as f:\n",
    "    writer = csv.DictWriter(f, dialect=\"unix\", fieldnames=[\"ITEM_ID\", \"CATEGORY\"])\n",
    "    writer.writeheader()\n",
    "    for i, item in enumerate(tqdm(\n",
    "        util.dataformat.data_folder_reader(\"data/raw/items\"),\n",
    "        total=n_items,\n",
    "        desc=\"Writing items CSV\",\n",
    "        mininterval=0.2,\n",
    "    )):\n",
    "        item_id = util.dataformat.get_item_id(item)\n",
    "        cats = item.get(\"category\", item.get(\"categories\", item.get(\"CATEGORY\", [])))\n",
    "        if isinstance(cats, str):\n",
    "            cats = cats.split(\"|\")\n",
    "        writer.writerow({\n",
    "            \"ITEM_ID\": item_id,\n",
    "            \"CATEGORY\": \"|\".join(cats[:3]),  # Only take up to the first 3 categories\n",
    "        })\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Using Amazon Rekognition\n",
    "\n",
    "In this alternative method, we'll fetch the advertised image URL (the first one, if there are multiple) of each product and run it through the standard public [Amazon Rekognition](https://aws.amazon.com/rekognition/) model.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <p>\n",
    "        <b>Warning:</b> Depending on the size of your dataset, this method could be <b>slow</b> and/or incur\n",
    "        significant <b>cost</b>.\n",
    "    </p>\n",
    "    <p>\n",
    "        Make sure you understand the\n",
    "        <a href=\"https://aws.amazon.com/rekognition/pricing/\">pricing of Amazon Rekognition</a>\n",
    "        before continuing with this method. For example processing 50k items in\n",
    "        <span class=\"code\">us-east-1</span>\n",
    "        could cost US&#36;50, excluding free tier allowances.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_cats_per_item = 3\n",
    "\n",
    "# We set this differently between the two methods, so you won't overwrite by default:\n",
    "items_filename = \"data/items-rekognition.csv\"\n",
    "\n",
    "with open(items_filename, \"w\") as f:\n",
    "    writer = csv.DictWriter(f, dialect=\"unix\", fieldnames=[\"ITEM_ID\", \"CATEGORY\"])\n",
    "    writer.writeheader()\n",
    "    for i, item in enumerate(tqdm(\n",
    "        util.dataformat.data_folder_reader(\"data/raw/items\"),\n",
    "        total=n_items,\n",
    "        desc=\"Analyzing item images\",\n",
    "        mininterval=0.2,\n",
    "    )):\n",
    "        item_id = util.dataformat.get_item_id(item)\n",
    "        imgurl = util.dataformat.get_item_imgurl(item)\n",
    "        if imgurl:\n",
    "            try:\n",
    "                img = requests.get(imgurl).content\n",
    "            except:\n",
    "                warnings.warn(f\"Couldn't get image for item ID {item_id}, URL {imgurl}\")\n",
    "                img = None\n",
    "            if not img:\n",
    "                labels = []\n",
    "            else:\n",
    "                try:\n",
    "                    rekresponse = rekognition.detect_labels(\n",
    "                        Image={ \"Bytes\" : img },\n",
    "                        MaxLabels=5,\n",
    "                        MinConfidence=60,\n",
    "                    )\n",
    "                except botoexceptions.ClientError as err:\n",
    "                    warnings.warn(\"Couldn't query Rekognition for item ID {}: {}\".format(\n",
    "                        item_id,\n",
    "                        err.response[\"Error\"][\"Code\"],\n",
    "                    ))\n",
    "                    rekresponse = None\n",
    "                if not rekresponse:\n",
    "                    labels = []\n",
    "                else:\n",
    "                    # Rekognition already sorts the detected labels by descending confidence, but we want to take the names\n",
    "                    # of any parent categories too if detected. So first build up a nested list [name, parent1, parent2] per\n",
    "                    # detection:\n",
    "            #         labels = [\n",
    "            #             [label[\"Name\"]] + [parent[\"Name\"] for parent in label[\"Parents\"]] for label in rekresponse[\"Labels\"]\n",
    "            #         ]\n",
    "                    labels = [\n",
    "                        [label[\"Name\"]] for label in rekresponse[\"Labels\"]\n",
    "                    ]\n",
    "                    # ...then flatten out into a list of strings, and keep only the first max_cats_per_item:\n",
    "                    labels = [item for sublist in labels for item in sublist][:max_cats_per_item]\n",
    "        else:\n",
    "            labels = []\n",
    "\n",
    "        writer.writerow({\n",
    "            \"ITEM_ID\": item_id,\n",
    "            \"CATEGORY\": \"|\".join(labels),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading the data\n",
    "\n",
    "Now that our local CSV is created, uploading the data to S3 and importing it to Amazon Personalize is just like for the **Interactions** dataset we provided first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3:\n",
    "s3.Object(os.environ[\"STAGING_BUCKET\"], items_filename).upload_file(items_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema in Personalize:\n",
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",  # Note different name vs 'Interactions'\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        { \"name\": \"ITEM_ID\", \"type\": \"string\" },\n",
    "        # Per the docs, our custom string field should be marked as 'categorical'\n",
    "        { \"name\": \"CATEGORY\", \"type\": \"string\", \"categorical\": True },\n",
    "    ],\n",
    "    \"version\": \"1.0\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name=f\"{os.environ['CF_STACK_NAME']}-schema-items\",\n",
    "        schema=json.dumps(items_schema)\n",
    "    )\n",
    "\n",
    "    items_schema_arn = create_schema_response[\"schemaArn\"]\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "\n",
    "except botoexceptions.ClientError as err:\n",
    "    # If the schema already exists, scrape the ARN from the message and check it:\n",
    "    if err.response[\"Error\"][\"Code\"] == \"ResourceAlreadyExistsException\":\n",
    "        warnings.warn(\n",
    "            \"Schema already exists.\\nScraping ARN from error message.\\n\"\n",
    "            \"To change the existing schema, delete it and re-create it.\"\n",
    "        )\n",
    "        msg = err.response[\"Error\"][\"Message\"]\n",
    "        items_schema_arn = msg[msg.index(\"arn:aws:personalize\"):].partition(\" \")[0]\n",
    "        # TODO: Deal with no way to delete schemas in Personalize console\n",
    "        personalize.delete_schema(schemaArn=items_schema_arn)\n",
    "        description = personalize.describe_schema(schemaArn=items_schema_arn)\n",
    "        print(description)\n",
    "    else:\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset:\n",
    "try:\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name=f\"{os.environ['CF_STACK_NAME']}-items\",\n",
    "        datasetType=\"ITEMS\",\n",
    "        datasetGroupArn=dataset_group_arn,\n",
    "        schemaArn=items_schema_arn,\n",
    "    )\n",
    "\n",
    "    items_dataset_arn = create_dataset_response[\"datasetArn\"]\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "except botoexceptions.ClientError as err:\n",
    "    # If the schema already exists, infer the ARN and check it:\n",
    "    if err.response[\"Error\"][\"Code\"] == \"ResourceAlreadyExistsException\":\n",
    "        warnings.warn(\n",
    "            \"Dataset already exists.\\n\"\n",
    "            \"To change the existing dataset's schema, delete it and re-create it.\"\n",
    "        )\n",
    "        items_dataset_arn = (\n",
    "            dataset_group_arn.replace(\":dataset-group/\", \":dataset/\")\n",
    "            + \"/ITEMS\"\n",
    "        )\n",
    "        description = personalize.describe_dataset(datasetArn=items_dataset_arn)\n",
    "        print(description)\n",
    "    else:  # Some other problem\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Importing from {items_filename}\")\n",
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    # (strftime %f is microseconds, so we trim 3 from the end for milliseconds)\n",
    "    jobName=f\"{os.environ['CF_STACK_NAME']}-items-{dt.now().strftime('%Y-%m-%d-%H-%M-%S-%f')[:-3]}\",\n",
    "    datasetArn=items_dataset_arn,\n",
    "    dataSource={\n",
    "        \"dataLocation\": f\"s3://{os.environ['STAGING_BUCKET']}/{items_filename}\"\n",
    "    },\n",
    "    roleArn=role_arn,  # Remember the IAM role we created earlier?\n",
    ")\n",
    "\n",
    "items_dataset_import_job_arn = create_dataset_import_job_response[\"datasetImportJobArn\"]\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-training the model\n",
    "\n",
    "To create a new metadata-aware model from our new dataset versions, first we need to check that the imports are finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_import_is_done(description):\n",
    "    status = description[\"datasetImportJob\"][\"status\"]\n",
    "    if status == \"ACTIVE\":\n",
    "        return True\n",
    "    elif \"FAILED\" in status:\n",
    "        raise ValueError(\n",
    "            f\"Wait ended with unexpected status '{status}':\\n{description}\"\n",
    "        )\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(f\"Waiting for dataset import {interactions_dataset_import_job_arn}...\")\n",
    "util.progress.polling_spinner(\n",
    "    lambda: personalize.describe_dataset_import_job(datasetImportJobArn=interactions_dataset_import_job_arn),\n",
    "    dataset_import_is_done,\n",
    "    fn_stringify_result = lambda desc: desc[\"datasetImportJob\"][\"status\"],\n",
    "    timeout_secs=60*60,\n",
    ")\n",
    "\n",
    "print(f\"Waiting for dataset import {items_dataset_import_job_arn}...\")\n",
    "util.progress.polling_spinner(\n",
    "    lambda: personalize.describe_dataset_import_job(datasetImportJobArn=items_dataset_import_job_arn),\n",
    "    dataset_import_is_done,\n",
    "    fn_stringify_result = lambda desc: desc[\"datasetImportJob\"][\"status\"],\n",
    "    timeout_secs=60*60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll select a **metadata-aware recipe** - since the base HRNN recipe doesn't make use of item (or user) metadata sets - and create a new solution and solution version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_recipe_arn = \"arn:aws:personalize:::recipe/aws-hrnn-metadata\"\n",
    "\n",
    "create_solution_response = personalize.create_solution(\n",
    "    name=f\"{os.environ['CF_STACK_NAME']}-soln-metadata\",\n",
    "    datasetGroupArn=dataset_group_arn,\n",
    "    recipeArn=metadata_recipe_arn,\n",
    ")\n",
    "\n",
    "metadata_solution_arn = create_solution_response[\"solutionArn\"]\n",
    "%store metadata_solution_arn\n",
    "print(json.dumps(create_solution_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_solution_version_response = personalize.create_solution_version(\n",
    "    solutionArn=metadata_solution_arn\n",
    ")\n",
    "\n",
    "metadata_solution_version_arn = create_solution_version_response[\"solutionVersionArn\"]\n",
    "print(json.dumps(create_solution_version_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if you'd like to resume with an existing solution version ARN taken from the console:\n",
    "#metadata_solution_version_arn = \"arn:aws:personalize:us-east-1:387269085412:solution/thewsfooda-soln-metadata/68311135\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we then just need to wait until our solution version is active (finished training): Then we'll be able to view its offline metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_version_is_ready(description):\n",
    "    status = description[\"solutionVersion\"][\"status\"]\n",
    "    if status == \"ACTIVE\":\n",
    "        return True\n",
    "    elif \"FAILED\" in status:\n",
    "        raise ValueError(\n",
    "            f\"Wait ended with unexpected status '{status}':\\n{description}\"\n",
    "        )\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(f\"Waiting for solution version {metadata_solution_version_arn}...\")\n",
    "util.progress.polling_spinner(\n",
    "    lambda: personalize.describe_solution_version(solutionVersionArn=metadata_solution_version_arn),\n",
    "    solution_version_is_ready,\n",
    "    fn_stringify_result = lambda desc: desc[\"solutionVersion\"][\"status\"],\n",
    "    timeout_secs=3*60*60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn=metadata_solution_version_arn\n",
    ")\n",
    "\n",
    "print(json.dumps(solution_metrics_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics (either here through the API, or as shown in the Amazon Personalize console) can help us compare the metadata-aware solution with our first-cut plain HRNN model.\n",
    "\n",
    "In our example (on the \"Grocery and Gourmet Food\" category), metric differences between metadata-enabled and plain HRNN recipes were pretty marginal.\n",
    "\n",
    "**Using Method 1** (Category data pulled through from source), we saw:\n",
    "\n",
    "- Exactly equal Precision@5 = 0.0144, and a nominally small uplift in nDCG@5 from 0.0662 to 0.0665.\n",
    "- However, the metadata model was able to deliver this near-identical result relevance with an uplift of coverage from 0.1706 to 0.2345\n",
    "- This means that a bigger proportion (23%) of our product catalogue appeared in recommendations without sacrificing relevance, which we might generally see as a positive thing.\n",
    "\n",
    "**Using Method 2** (Category data from Rekognition product image analysis), we saw:\n",
    "\n",
    "- Significantly noisier \"CATEGORY\" annotations, which is to be expected as we're using a general-purpose computer vision model on images of unknown quality, rather than taking the curated category labels from Amazon.com! E.g. for B00006FWVX `Food|Sweets|Confectionery` versus `Grocery & Gourmet Food|Cooking & Baking|Food Coloring`\n",
    "- ...But interestingly, some performance metrics were a little improved! Precision@5 up to 0.0146, nDCG@5 at 0.0707. Given the small size of these changes, they could easily be simple noise indicating that the metadata is not significantly contributing to these metrics.\n",
    "- Coverage at 0.2034 was higher than the base model, but not as good as method 1: This is more in line with our expectation that some kind of generalization power was derived from the extra data, but the quality of signal was lower in method 2 than method 1.\n",
    "\n",
    "In general it's possible to enrich Amazon Personalize datasets with metadata, and possible to derive this metadata from other ML and AI models where appropriate: But we need to keep in mind that:\n",
    "\n",
    "- Only **high-quality, useful fields** (that tell us information beyond what can already be seen from the correlations between items) are likely to cause big changes in our precision/relevance.\n",
    "- These offline metrics are only **part of the story**, and the expanded model may still deliver useful results in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the campaign\n",
    "\n",
    "There might be a few different ways we'd like to put our new solution version live to see real-world performance: For example by deploying to a separate \"canary\" campaign firts which only receives a subset of traffic... In this simple example though, we'll just update our existing campaign to point at the new solution version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_campaign_response = personalize.update_campaign(\n",
    "    campaignArn=hrnn_campaign_arn,\n",
    "    solutionVersionArn=metadata_solution_version_arn,\n",
    "    minProvisionedTPS=1,\n",
    ")\n",
    "print(json.dumps(update_campaign_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def campaign_is_ready(description):\n",
    "    status = description[\"campaign\"][\"status\"]\n",
    "    if status == \"ACTIVE\":\n",
    "        return True\n",
    "    elif \"FAILED\" in status:\n",
    "        raise ValueError(\n",
    "            f\"Wait ended with unexpected status '{status}':\\n{description}\"\n",
    "        )\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(f\"Waiting for campaign {hrnn_campaign_arn}...\")\n",
    "util.progress.polling_spinner(\n",
    "    lambda: personalize.describe_campaign(campaignArn=hrnn_campaign_arn),\n",
    "    campaign_is_ready,\n",
    "    fn_stringify_result = lambda desc: desc[\"campaign\"][\"status\"],\n",
    "    timeout_secs=20*60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and that's it! We:\n",
    "\n",
    "- Added some metadata fields on our **interactions** and **items** datasets to help Personalize understand correlations and generalize behaviours\n",
    "- Trained a new solution version using a metadata-aware recipe\n",
    "- Updated our deployed HRNN campaign to point at the new metadata-enriched model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Another way to optimize model performance is to tune the model **hyperparameters** while keeping the dataset the same.\n",
    "\n",
    "Amazon Personalize draws a distinction between [**hyperparameter optimization**](https://docs.aws.amazon.com/personalize/latest/dg/customizing-solution-config-hpo.html) (tuning the parameters of a recipe to optimize performance) and [**AutoML**](https://docs.aws.amazon.com/personalize/latest/dg/training-deploying-solutions.html) (also trying *multiple* recipes to see which one can perform best). However as discussed already the different Personalize recipes often **solve different use cases**: So we'll focus on HPO here as our primary interest in performance optimization, and ignore AutoML.\n",
    "\n",
    "To show HPO in action, we'll train another new solution using another new recipe: **HRNN-Coldstart**. HRNN-Coldstart works similarly to HRNN-Metadata (i.e. it's an HRNN-based recipe which is aware of the user and item metadata sets), but gives additional options to *force recommendation of new (unseen) items*.\n",
    "\n",
    "HRNN-Coldstart provides a range of [HPO-tunable parameters](https://docs.aws.amazon.com/personalize/latest/dg/native-recipe-hrnn-coldstart.html), which if we wanted we could [define explicit search ranges for](https://docs.aws.amazon.com/personalize/latest/dg/customizing-solution-config-hpo.html) in the HPO request: But here we'll just trigger a search over the default hyperparameter ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldstart_recipe_arn = \"arn:aws:personalize:::recipe/aws-hrnn-coldstart\"\n",
    "\n",
    "create_solution_response = personalize.create_solution(\n",
    "    name=f\"{os.environ['CF_STACK_NAME']}-soln-hpo\",\n",
    "    datasetGroupArn=dataset_group_arn,\n",
    "    performHPO=True,\n",
    "    performAutoML=False,  # As discussed above\n",
    "    recipeArn=coldstart_recipe_arn,\n",
    "    # TODO: Commentary on extra config\n",
    "    solutionConfig={\n",
    "        \"featureTransformationParameters\": {\n",
    "            \"cold_start_max_duration\": \"5.0\",\n",
    "            \"cold_start_max_interactions\": \"15\",\n",
    "            \"cold_start_relative_from\": \"latestItem\",\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "hpo_solution_arn = create_solution_response[\"solutionArn\"]\n",
    "%store hpo_solution_arn\n",
    "print(json.dumps(create_solution_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because HPO (and AutoML) are simply additional parameters in the solution specification - the process from here of training and deploying a solution is just as we've seen already!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_solution_version_response = personalize.create_solution_version(\n",
    "    solutionArn=hpo_solution_arn\n",
    ")\n",
    "\n",
    "hpo_solution_version_arn = create_solution_version_response[\"solutionVersionArn\"]\n",
    "print(json.dumps(create_solution_version_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Waiting for solution version {hpo_solution_version_arn}...\")\n",
    "util.progress.polling_spinner(\n",
    "    lambda: personalize.describe_solution_version(solutionVersionArn=hpo_solution_version_arn),\n",
    "    solution_version_is_ready,\n",
    "    fn_stringify_result = lambda desc: desc[\"solutionVersion\"][\"status\"],\n",
    "    timeout_secs=3*60*60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_campaign_response = personalize.update_campaign(\n",
    "    campaignArn=hrnn_campaign_arn,\n",
    "    solutionVersionArn=hpo_solution_version_arn,\n",
    "    minProvisionedTPS=1,\n",
    ")\n",
    "print(json.dumps(update_campaign_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Waiting for campaign {hrnn_campaign_arn}...\")\n",
    "util.progress.polling_spinner(\n",
    "    lambda: personalize.describe_campaign(campaignArn=hrnn_campaign_arn),\n",
    "    campaign_is_ready,\n",
    "    fn_stringify_result = lambda desc: desc[\"campaign\"][\"status\"],\n",
    "    timeout_secs=20*60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "In this series of notebooks we set up 3 different types of real-time recommendation engine on our \"AllStore\" website, and started to investigate performance optimization via metadata and hyperparameter tuning.\n",
    "\n",
    "While offline metrics (like coverage, precision @ N and nCDG reported in the Amazon Personalize console) are useful for drawing relative comparisons between models, the strong feedback loop between recommendation engines' output and user behaviour means the best way to understand the value of these solutions is via production pilot testing against measurable business outcomes!\n",
    "\n",
    "We hope these worked examples have helped you understand how to explore recommendation engines with Amazon Personalize, and see what the service can do for you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}